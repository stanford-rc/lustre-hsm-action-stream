# Copyright (C) 2025
#      The Board of Trustees of the Leland Stanford Junior University
# Written by Stephane Thiell <sthiell@stanford.edu>
#
# Licensed under GPL v3 (see https://www.gnu.org/licenses/).
"""
A public consumer API for the Lustre HSM Action Stream.

This module provides a high-level interface for discovering and reading events
from the per-MDT Redis streams generated by the hsm-action-shipper.

Example Usage:
--------------
from lustre_hsm_action_stream.consumer import StreamReader

# Create a reader for streams with the prefix 'hsm:actions'
reader = StreamReader(host='localhost', port=6379, db=1, prefix='hsm:actions')

# Continuously tail events from now on
print("Tailing new events...")
try:
    for event in reader.events():
        print(f"Stream: {event.stream}, ID: {event.id}, Data: {event.data}")
except KeyboardInterrupt:
    print("Stopped.")

# Or, replay all historical events and then stop
print("\\nReplaying all historical events...")
for event in reader.events(from_beginning=True, block_ms=500):
    if not event:  # The generator yields None when history is fully replayed
        break
    print(f"Historical Event -> Stream: {event.stream}, Data: {event.data}")
"""

import redis
import json
import time
import logging
from collections import namedtuple

# A structured object to represent a single event read from a stream.
# - stream: The name of the Redis stream the event came from.
# - id: The unique message ID of the event in the stream.
# -  The deserialized JSON payload of the event.
StreamEvent = namedtuple('StreamEvent', ['stream', 'id', 'data'])

class StreamReader:
    """
    A high-level class for discovering and consuming events from multiple
    Lustre HSM action streams in Redis.
    """

    def __init__(self, host, port, db, prefix, discovery_interval=60, **redis_kwargs):
        """
        Initializes the StreamReader.

        Args:
            host (str): Redis server host.
            port (int): Redis server port.
            db (int): Redis database number.
            prefix (str): The prefix used to discover streams (e.g., 'hsm:actions').
            discovery_interval (int): How often, in seconds, to re-scan Redis
                                      for new streams.
            **redis_kwargs: Additional keyword arguments for the redis.Redis client.
        """
        self.redis_config = {'host': host, 'port': port, 'db': db, **redis_kwargs}
        self.prefix = prefix
        self.redis_client = None
        self.streams = []
        self.is_connected = False
        self.discovery_interval = discovery_interval
        self.last_discovery_ts = 0

    def _connect(self):
        """Establishes and pings the Redis connection."""
        if self.redis_client:
            try:
                self.redis_client.ping()
                self.is_connected = True
                return
            except (redis.exceptions.ConnectionError, redis.exceptions.TimeoutError):
                logging.warning("Redis connection lost. Attempting to reconnect...")
                self.redis_client = None
                self.is_connected = False

        while True:
            try:
                logging.info(f"Connecting to Redis at {self.redis_config['host']}:{self.redis_config['port']}...")
                self.redis_client = redis.Redis(**self.redis_config, decode_responses=False)
                self.redis_client.ping()
                self.is_connected = True
                logging.info("Successfully connected to Redis.")
                return
            except (redis.exceptions.ConnectionError, redis.exceptions.TimeoutError) as e:
                logging.error(f"Redis connection failed: {e}. Retrying in 5 seconds.")
                self.is_connected = False
                time.sleep(5)

    def discover_streams(self):
        """
        Scans Redis for streams matching the configured prefix.
        """
        self._connect()
        discovered = []
        try:
            for key in self.redis_client.scan_iter(match=f"{self.prefix}:*"):
                if self.redis_client.type(key) == b'stream':
                    discovered.append(key.decode('utf-8'))
            # Only log if the set of discovered streams has actually changed.
            if set(self.streams) != set(discovered):
                logging.info(f"Stream discovery found {len(discovered)} streams: {sorted(discovered)}")
                self.streams = sorted(discovered)

            self.last_discovery_ts = time.time() # Update timestamp after every successful scan

        except Exception as e:
            logging.error(f"Failed to scan for streams with prefix '{self.prefix}': {e}")
            self.streams = []
        return self.streams

    def events(self, from_beginning=False, block_ms=0):
        """
        A generator that discovers and yields events from all matching streams.

        This method will block and continuously yield new events as they arrive.
        It automatically handles reconnection and re-discovery of streams.

        Args:
            from_beginning (bool): If True, starts reading from the beginning
                of each stream's history. If False, starts from the latest event.
            block_ms (int): Milliseconds to block waiting for new events.
                If 0, blocks indefinitely. If set to a value > 0, the generator
                will yield `None` after the timeout if no new events arrive,
                which is useful for knowing when a historical replay is complete.

        Yields:
            StreamEvent or None: A `StreamEvent` namedtuple for each new event,
                or `None` if the blocking timeout is reached without new data.
        """
        # Initial discovery before the main loop starts
        self.discover_streams()

        last_ids = {stream: '0-0' if from_beginning else '$' for stream in self.streams}

        while True:
            try:
                self._connect()

                # --- Time-based re-discovery logic ---
                if time.time() - self.last_discovery_ts > self.discovery_interval:
                    current_streams = set(self.streams)
                    self.discover_streams() # Re-scans and updates self.streams
                    # If new streams were found, add them to our read list
                    if set(self.streams) != current_streams:
                        for stream in self.streams:
                            last_ids.setdefault(stream, '0-0' if from_beginning else '$')

                if not self.streams:
                    logging.warning("No streams found to listen to. Waiting...")
                    time.sleep(self.discovery_interval)
                    continue

                # Use a shorter block time to allow the discovery loop to run periodically
                # even if there are no events.
                effective_block_ms = block_ms if block_ms > 0 else 5000

                response = self.redis_client.xread(last_ids, count=100, block=effective_block_ms)

                if not response:
                    # If we timed out, yield None only if the caller expects it (block_ms > 0)
                    if block_ms > 0:
                        yield None
                    continue # Loop again to check for new streams or more events

                for stream_bytes, messages in response:
                    stream_name = stream_bytes.decode('utf-8')
                    for msg_id_bytes, event_data in messages:
                        msg_id = msg_id_bytes.decode('utf-8')
                        try:
                            data = json.loads(event_data[b'data'])
                            yield StreamEvent(stream=stream_name, id=msg_id, data=data)
                        except (json.JSONDecodeError, KeyError) as e:
                            logging.warning(f"Could not parse event {msg_id} in {stream_name}: {e}")
                        last_ids[stream_name] = msg_id_bytes

            except (redis.exceptions.ConnectionError, redis.exceptions.TimeoutError):
                logging.warning("Connection error in event loop. Will attempt to reconnect.")
                self.is_connected = False
                time.sleep(5)
            except KeyboardInterrupt:
                logging.info("Interrupted. Stopping event generator.")
                break
            except Exception as e:
                logging.error(f"An unexpected error occurred in event loop: {e}", exc_info=True)
                time.sleep(5)
